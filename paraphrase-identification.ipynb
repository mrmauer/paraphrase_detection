{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "## Training\n",
    "For every training pair, selected whether to add noise and change label to 0 (ie not paraphrase) with rpobability 0.5. If adding noise, selected whether to pick random sentence/document to replace one from the pair or intro duce noise into one of the documents with probability of 0.5 for each. If adding noise, use a defined parament, learning noise or LN, which defaults to 10%, and select LN% of of one of the sentence's token to be replaced with another token selected uniformly at random from the embedding vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from textdistance import jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext import vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1285f43f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(12144)\n",
    "torch.manual_seed(31190)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pipeline' from '/Users/matthewmauer/NLP/hw/4/pipeline.py'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_training = []\n",
    "with open('data/train.tsv', 'r') as f:\n",
    "    for line in f:\n",
    "        a, b = line.strip(\"\\n\").split(\"\\t\")\n",
    "        raw_training.append((a, b))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dev = []\n",
    "with open('data/dev+devtest/dev.tsv', 'r') as f:\n",
    "    for line in f:\n",
    "        t1, t2, y = line.strip('\\n').split('\\t')\n",
    "        raw_dev.append((t1, t2, int(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEVTEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_devtest = []\n",
    "with open('data/dev+devtest/devtest.tsv', 'r') as f:\n",
    "    for line in f:\n",
    "        t1, t2, y = line.strip('\\n').split('\\t')\n",
    "        raw_devtest.append((t1, t2, int(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Easy\" Kaggle Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_kaggle_test = {}\n",
    "with open('data/test_no_labels.tsv', 'r') as f:\n",
    "    for line in f:\n",
    "        i, a, b = line.strip(\"\\n\").split(\"\\t\")\n",
    "        raw_kaggle_test[i] = (a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Hard\" DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dev_hard = []\n",
    "with open('data/heldout-hard/dev.hard.tsv', 'r') as f:\n",
    "    for line in f:\n",
    "        t1, t2, y = line.strip('\\n').split('\\t')\n",
    "        raw_dev_hard.append((t1, t2, int(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_devtest_hard = []\n",
    "with open('data/heldout-hard/devtest.hard.tsv', 'r') as f:\n",
    "    for line in f:\n",
    "        t1, t2, y = line.strip('\\n').split('\\t')\n",
    "        raw_devtest_hard.append((t1, t2, int(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Hard\" Kaggle Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_kaggle_test_hard = {}\n",
    "with open('data/test_no_labels-hard.tsv', 'r') as f:\n",
    "    for line in f:\n",
    "        i, a, b = line.strip(\"\\n\").split(\"\\t\")\n",
    "        raw_kaggle_test_hard[i] = (a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser and Word Embeddings\n",
    "* Using the basic English parser from spaCy for tokenization.\n",
    "* Using the 300-dimension, 6B Wikipedia word embeddings [Glove](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = English()\n",
    "embeddings300 = vocab.GloVe(name='6B', dim=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline LSTM Model\n",
    "* Siames LSTM\n",
    "* The two vectors from the LSTM are joined by an absolute difference elementwise\n",
    "* 300 dimension embeddings\n",
    "* randomly initialized and trained h0, c0 (ie initial hidden and cell vectors)\n",
    "* no manual features\n",
    "* learning noise: 0.1\n",
    "* No additional between joining the LSTM outputs and the final classifier layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 50000 training samples, accuracy is 0.7396 on the DEV data.\n",
      "After 50000 training samples, accuracy is 0.7292 on the DEVTEST data.\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "580.8 seconds has been spent training.\n",
      "The best score on DEVTEST data was 0.7292 after 50000 training samples.\n"
     ]
    }
   ],
   "source": [
    "net = pipeline.AbsDiffSiamese()\n",
    "absdiffsiamese = pipeline.ParaphraseClassifier(\n",
    "    net=net,\n",
    "    embeddings=embeddings300,\n",
    "    parser=parser\n",
    ")\n",
    "absdiffsiamese.train(raw_training[:50000], raw_dev, raw_devtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 100000 training samples, accuracy is 0.7323 on the DEV data.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f70f12eaea63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mabsdiffsiamese\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_training\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_devtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/NLP/hw/4/pipeline.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, XYtrain, XYdev, XYtest, epochs, verbose)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0;31m# backprop and update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-sandbox/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-sandbox/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "absdiffsiamese.train(raw_training[50000:200000], raw_dev, raw_devtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping hould be performed at 200k samples for the above model. The model converges to a local minimum by then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# absdiffsiamese.train(raw_training[200000:], raw_dev, raw_devtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy on DEVTEST after early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7478260869565218"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absdiffsiamese.test(raw_devtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy on \"Easy\" Kaggle set\n",
    "__0.865__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/easy/absdiff.csv\", 'w', newline='') as f:\n",
    "#     writer = csv.writer(f, delimiter=',')\n",
    "#     writer.writerow([\"ID\", \"Category\"])\n",
    "#     for i in raw_kaggle_test:\n",
    "#         prediction = absdiffsiamese.predict(raw_kaggle_test[i])\n",
    "#         writer.writerow([i, prediction])\n",
    "\n",
    "pipeline.write_results(\"results/easy/absdiff.csv\", absdiffsiamese, raw_kaggle_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy on Hard DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.432"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absdiffsiamese.test(raw_dev_hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy on \"Hard\" Kaggle set\n",
    "__0.443__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/hard/absdiff.csv\", 'w', newline='') as f:\n",
    "#     writer = csv.writer(f, delimiter=',')\n",
    "#     writer.writerow([\"ID\", \"Category\"])\n",
    "#     for i in raw_kaggle_test_hard:\n",
    "#         prediction = absdiffsiamese.predict(raw_kaggle_test_hard[i])\n",
    "#         writer.writerow([i, prediction])\n",
    "\n",
    "pipeline.write_results(\"results/hard/absdiff.csv\", absdiffsiamese, raw_kaggle_test_hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements for the Hard data\n",
    "* Use of elementwise product instead of absolute difference for the LSTM join.\n",
    "* Lowering of the learning noise to 5% to detect smaller differences in the sentence pair.\n",
    "* Introduction of POS in a separate Siamese LSTM.\n",
    "    * Several negative samples in the hard data have the subject and object flipped...\n",
    "* Manual features.\n",
    "* Use only the hard dev and devtest data for training.\n",
    "    * With smaller embedding dimensions to avoid overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_devtest_hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CHANGES__\n",
    "* Learning noise: 0.05\n",
    "* Product Siamese join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 50000 training samples, accuracy is 0.6834 on the DEV data.\n",
      "After 50000 training samples, accuracy is 0.463 on the DEVTEST data.\n",
      "After 100000 training samples, accuracy is 0.7017 on the DEV data.\n",
      "After 100000 training samples, accuracy is 0.463 on the DEVTEST data.\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "1207.7 seconds has been spent training.\n",
      "The best score on DEVTEST data was 0.463 after 50000 training samples.\n"
     ]
    }
   ],
   "source": [
    "net = pipeline.ProductSiamese()\n",
    "product_siamese = pipeline.ParaphraseClassifier(\n",
    "    net=net,\n",
    "    embeddings=embeddings300,\n",
    "    parser=parser,\n",
    "    learning_noise=0.05\n",
    ")\n",
    "product_siamese.train(raw_training[:100000], raw_dev, raw_dev_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 150000 training samples, accuracy is 0.7078 on the DEV data.\n",
      "After 200000 training samples, accuracy is 0.6883 on the DEV data.\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "2414.2 seconds has been spent training.\n",
      "The best score on DEVTEST data was 0.484 after 100000 training samples.\n"
     ]
    }
   ],
   "source": [
    "product_siamese.train(raw_training[100000:200000], raw_dev, raw_dev_hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping hould be performed at 100k samples for the above model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6993788819875777"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_siamese.test(raw_devtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.471"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_siamese.test(raw_devtest_hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__RESULTS__  \n",
    "Early stopping should be performed after 100k samples. There seemed to be modest impprovement at that stage, but the gains rapidly decline after 100k samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CHANGES__\n",
    "The model from above with the four manual features:\n",
    "* TfIdf cosine similarity.\n",
    "* 5-character-gram Jaccard score.\n",
    "* The absolute difference in count of negation terms. (Scaled down by a factor of 10.)\n",
    "* The number of numerical discrepencies. (Scaled down by a factor of 10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the engineer\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "tfidf_vec.fit(itertools.chain(*random.choices(raw_training, k=100000)))\n",
    "\n",
    "engineer = pipeline.FeatureEngineer(\n",
    "    tfidf_vectorizer=tfidf_vec\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 50000 training samples, accuracy is 0.6932 on the DEV data.\n",
      "After 50000 training samples, accuracy is 0.433 on the DEVTEST data.\n",
      "After 100000 training samples, accuracy is 0.665 on the DEV data.\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "1421.5 seconds has been spent training.\n",
      "The best score on DEVTEST data was 0.433 after 50000 training samples.\n"
     ]
    }
   ],
   "source": [
    "# init the NN\n",
    "net_manual = net = pipeline.ProductSiamese(n_man_features=4)\n",
    "\n",
    "# init the full model\n",
    "# use weight decay to prevent overfitting to the manual features\n",
    "product_siamese_manual = pipeline.ParaphraseClassifier(\n",
    "    net=net_manual,\n",
    "    embeddings=embeddings300,\n",
    "    parser=parser,\n",
    "    learning_noise=0.05,\n",
    "    feature_engineer=engineer,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# train the model on the first 100k samples using DEV and the hard DEV for early stopping\n",
    "product_siamese_manual.train(raw_training[:100000], raw_dev, raw_dev_hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Hard Dev and Devtest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1000 training samples, accuracy is 0.592 on the DEV data.\n",
      "After 2000 training samples, accuracy is 0.596 on the DEV data.\n",
      "After 3000 training samples, accuracy is 0.609 on the DEV data.\n",
      "After 4000 training samples, accuracy is 0.606 on the DEV data.\n",
      "After 5000 training samples, accuracy is 0.59 on the DEV data.\n",
      "After 6000 training samples, accuracy is 0.589 on the DEV data.\n",
      "After 7000 training samples, accuracy is 0.592 on the DEV data.\n",
      "After 8000 training samples, accuracy is 0.583 on the DEV data.\n",
      "After 9000 training samples, accuracy is 0.581 on the DEV data.\n",
      "After 10000 training samples, accuracy is 0.574 on the DEV data.\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "284.4 seconds has been spent training.\n",
      "The best score on DEV data was 0.609 after 3000 training samples over 2 epochs.\n"
     ]
    }
   ],
   "source": [
    "product_siamese_manual_hard = pipeline.SupervisedParaphraseClassifier(\n",
    "    net=net_manual,\n",
    "    embeddings=embeddings300,\n",
    "    parser=parser,\n",
    "    learning_noise=0.05,\n",
    "    feature_engineer=engineer,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# train the model on the first 100k samples using DEV and the hard DEV for early stopping\n",
    "product_siamese_manual_hard.train(raw_dev_hard, raw_devtest_hard, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ADJUSTMENT__\n",
    "To handle overfitting and collapsing gradients, we train with cross validation. Training 2 epochs on one dataset while testing on the other, and then swap the training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__WITHOUT Manual Features__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1000 training samples, accuracy is 0.546 on the DEV data.\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "24.2 seconds has been spent training.\n",
      "The best score on DEV data was 0.546 after 1000 training samples over 2 epochs.\n",
      "After 2000 training samples, accuracy is 0.567 on the DEV data.\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "51.4 seconds has been spent training.\n",
      "The best score on DEV data was 0.567 after 2000 training samples over 2 epochs.\n",
      "After 3000 training samples, accuracy is 0.546 on the DEV data.\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "75.1 seconds has been spent training.\n",
      "The best score on DEV data was 0.567 after 2000 training samples over 2 epochs.\n",
      "After 4000 training samples, accuracy is 0.567 on the DEV data.\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "98.0 seconds has been spent training.\n",
      "The best score on DEV data was 0.567 after 2000 training samples over 2 epochs.\n",
      "After 5000 training samples, accuracy is 0.546 on the DEV data.\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "125.8 seconds has been spent training.\n",
      "The best score on DEV data was 0.567 after 2000 training samples over 2 epochs.\n",
      "After 6000 training samples, accuracy is 0.567 on the DEV data.\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "153.9 seconds has been spent training.\n",
      "The best score on DEV data was 0.567 after 2000 training samples over 2 epochs.\n"
     ]
    }
   ],
   "source": [
    "net = pipeline.ProductSiamese()\n",
    "\n",
    "product_siamese_hard = pipeline.SupervisedParaphraseClassifier(\n",
    "    net=net,\n",
    "    embeddings=embeddings300,\n",
    "    parser=parser\n",
    ")\n",
    "\n",
    "for i in range(6):\n",
    "    if i%2==0:\n",
    "        product_siamese_hard.train(raw_dev_hard, raw_devtest_hard, epochs=1)\n",
    "    else:\n",
    "        product_siamese_hard.train(raw_devtest_hard, raw_dev_hard, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__RESULTS__  \n",
    "The simple model didn't appear to learn anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__WITH Manual Features__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1000 training samples, accuracy is 0.57 on the DEV data.\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "30.0 seconds has been spent training.\n",
      "The best score on DEV data was 0.57 after 1000 training samples over 2 epochs.\n",
      "After 2000 training samples, accuracy is 0.7 on the DEV data.\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "61.1 seconds has been spent training.\n",
      "The best score on DEV data was 0.7 after 2000 training samples over 2 epochs.\n",
      "After 3000 training samples, accuracy is 0.589 on the DEV data.\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "93.4 seconds has been spent training.\n",
      "The best score on DEV data was 0.7 after 2000 training samples over 2 epochs.\n",
      "After 4000 training samples, accuracy is 0.714 on the DEV data.\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "121.7 seconds has been spent training.\n",
      "The best score on DEV data was 0.714 after 4000 training samples over 2 epochs.\n",
      "After 5000 training samples, accuracy is 0.624 on the DEV data.\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "152.0 seconds has been spent training.\n",
      "The best score on DEV data was 0.714 after 4000 training samples over 2 epochs.\n",
      "After 6000 training samples, accuracy is 0.718 on the DEV data.\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "183.3 seconds has been spent training.\n",
      "The best score on DEV data was 0.718 after 6000 training samples over 2 epochs.\n"
     ]
    }
   ],
   "source": [
    "product_siamese_manual_hard = pipeline.SupervisedParaphraseClassifier(\n",
    "    net=net_manual,\n",
    "    embeddings=embeddings300,\n",
    "    parser=parser,\n",
    "    learning_noise=0.05,\n",
    "    feature_engineer=engineer,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "for i in range(6):\n",
    "    if i%2==0:\n",
    "        product_siamese_manual_hard.train(raw_dev_hard, raw_devtest_hard, epochs=1)\n",
    "    else:\n",
    "        product_siamese_manual_hard.train(raw_devtest_hard, raw_dev_hard, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4682151589242054"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_siamese_manual_hard.test(raw_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/hard/product_siamese_manual_hard.csv\", 'w', newline='') as f:\n",
    "#     writer = csv.writer(f, delimiter=',')\n",
    "#     writer.writerow([\"ID\", \"Category\"])\n",
    "#     for i in raw_kaggle_test_hard:\n",
    "#         prediction = product_siamese_manual_hard.predict(raw_kaggle_test_hard[i])\n",
    "#         writer.writerow([i, prediction])\n",
    "\n",
    "pipeline.write_results(\"results/hard/product_siamese_manual_hard.csv\", product_siamese_manual_hard, raw_kaggle_test_hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__RESULTS__  \n",
    "It appears that the bulk of the predictive power of the model trained on the hard dev and devtest data comes from the manual features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Model\n",
    "Attempt a Naive Bayes model (Gaussian dist) with the 4 manual features using the entire hard DEV and DEVTEST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "engineered_training_hard = []\n",
    "\n",
    "for obs in raw_dev_hard + raw_devtest_hard:\n",
    "    doc1, doc2, y = obs\n",
    "    x = engineer.construct_features((doc1, doc2))\n",
    "    engineered_training_hard.append(x + [y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_training_hard_matrix = np.array(engineered_training_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_hard = GaussianNB()\n",
    "nb_hard.fit(X = engineered_training_hard_matrix[:,:-1], y = engineered_training_hard_matrix[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4445"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_hard.score(X = engineered_training_hard_matrix[:,:-1], y = engineered_training_hard_matrix[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9898589106597186, 0.7727272727272727, 0.0, 0.0, 1]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(engineered_training_hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__RESULTS__  \n",
    "The model learned little to nothing. The primary feature, TF-IDF cosine similarity, has very little variance in the hard dataset. Most of the difference between positive and negative samples is the result of reordering nouns and verbs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Model\n",
    "* For the easy data.\n",
    "* Train a simple Siamese LSTM (product join) with learning noise=0.05.\n",
    "* Use the outputs (without the softmax) of the Siamese LSTM as two inputs alongside the four manual features for a Naive Bayes model (Gaussian distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pipeline' from '/Users/matthewmauer/NLP/hw/4/pipeline.py'>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training on 818 training samples and 4.644663095474243 seconds of training, the stacked model has an accuracy of 0.8031784841075794 on the training data.\n"
     ]
    }
   ],
   "source": [
    "stacked_model = pipeline.StackedClassifier(\n",
    "    siamese_classifier = absdiffsiamese,\n",
    "    feature_engineer = engineer,\n",
    "    super_model = GaussianNB()\n",
    ")\n",
    "\n",
    "stacked_model.train(raw_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4434166441744399, 0.5, 0.0, 0.0, -0.7320107221603394, 0.6121735572814941, 1]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_model.processed_training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.write_results(\"results/easy/stacked.csv\", stacked_model, raw_kaggle_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__RESULTS__  \n",
    "__0.899__ accuracy on the easy Kaggle data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
